{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Function to extract leaf from image\n",
    "def extract_leaf(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(\"Error opening image:\", image_path)\n",
    "        return None\n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    # Convert to HSV and create mask for green regions\n",
    "    hsv = cv2.cvtColor(img_np, cv2.COLOR_RGB2HSV)\n",
    "    lower_green = np.array([10, 20, 20])\n",
    "    upper_green = np.array([85, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    \n",
    "    # Smooth and refine the mask\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Extract largest contour (leaf)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        leaf_mask = np.zeros_like(mask)\n",
    "        cv2.drawContours(leaf_mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "        leaf = cv2.bitwise_and(img_np, img_np, mask=leaf_mask)\n",
    "        return Image.fromarray(leaf)\n",
    "    else:\n",
    "        return img  # If no leaf is found, return original image\n",
    "\n",
    "# Custom Dataset to Load Both Original and Extracted Images\n",
    "class HybridPlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        # Read class names\n",
    "        classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "        # Store paths for both original and extracted images\n",
    "        for class_name in classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    img_path = os.path.join(class_path, img_name)\n",
    "                    self.data.append((img_path, self.class_to_idx[class_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load extracted leaf version\n",
    "        leaf_image = extract_leaf(img_path)\n",
    "        if leaf_image is None:\n",
    "            leaf_image = original_image  # Fallback in case of error\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            original_image = self.transform(original_image)\n",
    "            leaf_image = self.transform(leaf_image)\n",
    "        \n",
    "        # Concatenate both images along the channel dimension (6 channels: 3 original + 3 extracted)\n",
    "        hybrid_image = torch.cat((original_image, leaf_image), dim=0)\n",
    "        \n",
    "        return hybrid_image, label\n",
    "\n",
    "base_dir = \"plant_disease_data\"\n",
    "train_dir = f\"{base_dir}/train\"\n",
    "valid_dir = f\"{base_dir}/valid\"\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = HybridPlantDiseaseDataset(train_dir, transform=transform)\n",
    "valid_dataset = HybridPlantDiseaseDataset(valid_dir, transform=transform)\n",
    "\n",
    "# Initialize data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained ResNet18 model (modify first layer for 6-channel input)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Adjust for 6-channel input\n",
    "model.fc = nn.Linear(model.fc.in_features, len(train_dataset.class_to_idx))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop with progress tracking\n",
    "epochs = 15\n",
    "train_losses, valid_losses, train_accs, valid_accs = [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct, total, running_loss = 0, 0, 0\n",
    "    total_batches = len(train_loader)  # Total number of batches\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Progress tracking\n",
    "        progress = (batch_idx + 1) / total_batches\n",
    "        progress_bar = \"=\" * int(20 * progress) + \" \" * (20 - int(20 * progress))\n",
    "        print(f\"\\rProcessing batch {batch_idx+1}/{total_batches} [{progress_bar}] {int(progress * 100)}% done\", end=\"\")\n",
    "\n",
    "    train_loss = running_loss / total_batches\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total, running_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    valid_loss = running_loss / len(valid_loader)\n",
    "    valid_acc = correct / total\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    print(f\"Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(valid_losses, label=\"Valid Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(valid_accs, label=\"Valid Acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model_path = \"plant_disease_resnet18.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Move to Google Drive if mounted\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "torch.save(model.state_dict(), \"/content/drive/My Drive/\" + model_path)\n",
    "\n",
    "from google.colab import files\n",
    "files.download(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(train_dataset.class_to_idx))\n",
    "model.load_state_dict(torch.load(\"plant_disease_resnet18.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
